import cv2
from tqdm import tqdm
import numpy as np
import scipy


def torch_to_cv2(images):
    """
    Converts a collection of PyTorch images generated by a GAN to cv2-compatible format
    :param images:
    :return:
    """
    for i in range(len(images)):
        cur_imgs = images[i]
        cur_imgs = cur_imgs.transpose((0, 2, 3, 1))[:, :, :, ::-1].copy()
        cur_imgs = np.uint8(np.clip(((cur_imgs + 1) / 2.0) * 256, 0, 255))
        images[i] = cur_imgs
    images = np.concatenate(images)
    return images


def add_logo(logo, images):
    """
    Adds a logo to a collection of frames
    :param logo:
    :param images:
    :return:
    """
    logo = cv2.resize(logo, (80, 50))

    for i in range(len(images)):
        # Add white overlay
        image = images[i]
        overlay = image.copy()

        x_offset, y_offset = 420, 450
        logo_region = overlay[y_offset:y_offset +
                              logo.shape[0], x_offset:x_offset + logo.shape[1]]
        logo_region[logo < 35] = 0
        logo_region[logo > 200] = 255
        overlay[y_offset:y_offset + logo.shape[0],
                x_offset:x_offset + logo.shape[1]] = logo_region

        images[i] = cv2.addWeighted(overlay, 0.3, image, 1 - 0.3, 0)


def add_debug_info(images, predictions, song_words, stylizer_sentiments):
    """
    Adds the some interesting debug info to a collection of frames
    :param images:
    :param predictions:
    :param song_words:
    :param stylizer_sentiments:
    :return:
    """

    for i in range(len(images)):
        # Add white overlay
        image = images[i]
        overlay = image.copy()

        # Rectangle parameters
        x, y, w, h = 1, 10, 512, 60
        if len(stylizer_sentiments) > 0:
            h = 80
        cv2.rectangle(overlay, (x, y), (x + w, y + h), (255, 255, 255), -1)
        images[i] = cv2.addWeighted(overlay, 0.5, image, 1 - 0.5, 0)

        # Put info
        cv2.putText(images[i], "Sound sentiment: (%3.2f, %3.2f)" % (predictions[i][0], predictions[i][1]),
                    (2, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, 255, 2)
        cv2.putText(images[i], "GAN Class: %s" % song_words[i],
                    (2, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.6, 255, 2)
        if len(stylizer_sentiments) > 0:
            cv2.putText(images[i], "Stylizer Sentiment: %s" % stylizer_sentiments[i], (2, 80),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, 255, 2)


def generate_video(images, output_path='out.avi', frames_per_image=10, transition_frames=4, subsampling=4):
    corrected_fps = int(2 * frames_per_image / subsampling)

    if int(2 * frames_per_image / subsampling) != 2 * frames_per_image / subsampling:
        print("Subsampling does not lead to synced video! Check frames_per_image and subsampling")
        print(" frames per image: ", frames_per_image)
        print(" subsampling per image: ", subsampling)
        assert False

    print("FPS: ", corrected_fps)

    out = cv2.VideoWriter(output_path + ".avi", cv2.VideoWriter_fourcc(*'DIVX'), corrected_fps,
                          (images.shape[1], images.shape[2]))

    frames_per_image = np.int(frames_per_image)

    # Save 5 shots
    for i in range(5):
        cv2.imwrite(output_path + '_' + str(i) + '.jpeg',
                    images[np.random.randint(0, len(images))])

    # Write the first frame for the appropriate time
    for j in range(frames_per_image):
        out.write(images[0])

    for i in tqdm(range(images.shape[0] - 1)):
        factor = 0

        # if power_features is not None:
        #     cur_features = power_features[i*subsampling:(i+1)*subsampling]
        #     transition_factors = scipy.signal.resample(cur_features, transition_frames)
        #     transition_factors = transition_factors / np.sum(transition_factors)
        #     # transition_factors = np.ones((transition_frames, )) / transition_frames
        # else:
        transition_factors = np.ones((transition_frames, )) / transition_frames

        # Write the transition frames
        for j in range(transition_frames):
            out.write(np.uint8((1 - factor) *
                      images[i] + factor * images[i + 1]))
            factor += transition_factors[j]

        # Write the remaining frames
        for j in range(frames_per_image - transition_frames):
            out.write(images[i + 1])

    # Fill the last portion of b (clip when muxing if needed)
    for j in range(frames_per_image - 1):
        out.write(images[-1])

    out.release()
